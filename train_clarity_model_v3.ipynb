{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ielrR4lXkguu",
        "outputId": "83c61f41-5a08-4fcc-dd1c-bda823626839"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "etnZ2OnrjnLN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Load & clean dataset\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "ds = load_dataset(\"ailsntua/QEvasion\")\n",
        "\n",
        "# remove empty rows\n",
        "ds = ds.filter(lambda x: bool(x[\"interview_question\"]) and bool(x[\"interview_answer\"]))\n",
        "\n",
        "# map labels\n",
        "clarity_labels = sorted(set(ds[\"train\"][\"clarity_label\"]))\n",
        "evasion_labels = sorted(set(ds[\"train\"][\"evasion_label\"]))\n",
        "\n",
        "clarity2id = {c: i for i, c in enumerate(clarity_labels)}\n",
        "id2clarity = {i: c for c, i in clarity2id.items()}\n",
        "evasion2id = {c: i for i, c in enumerate(evasion_labels)}\n",
        "id2evasion = {i: c for c, i in evasion2id.items()}\n",
        "\n",
        "print(\"Clarity labels:\", clarity2id)\n",
        "print(\"Evasion labels:\", evasion2id)"
      ],
      "metadata": {
        "id": "1H2eHptyjuFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969f162e-47f0-4e23-ed02-388c8b3805ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clarity labels: {'Ambivalent': 0, 'Clear Non-Reply': 1, 'Clear Reply': 2}\n",
            "Evasion labels: {'Claims ignorance': 0, 'Clarification': 1, 'Declining to answer': 2, 'Deflection': 3, 'Dodging': 4, 'Explicit': 5, 'General': 6, 'Implicit': 7, 'Partial/half-answer': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "\n",
        "def preprocess(batch):\n",
        "    \"\"\"Use HF's built-in text pair encoding (question, answer).\"\"\"\n",
        "\n",
        "    enc = tokenizer(\n",
        "        batch[\"interview_question\"],\n",
        "        batch[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    enc[\"labels\"] = [\n",
        "        (\n",
        "            clarity2id[c] if c in clarity2id else 0,\n",
        "            evasion2id[e] if e in evasion2id else 0\n",
        "        )\n",
        "        for c, e in zip(batch[\"clarity_label\"], batch[\"evasion_label\"])\n",
        "    ]\n",
        "\n",
        "    return enc\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "encoded = ds.map(preprocess, batched=True)\n",
        "\n",
        "# keep only model-required columns\n",
        "encoded = encoded.remove_columns(\n",
        "    [c for c in ds[\"train\"].column_names if c not in [\"labels\"]]\n",
        ")\n",
        "\n",
        "encoded.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "LTz2t_O8jxzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b65b9f8-841b-486c-f73a-5cd674599254"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. Multi-task model\n",
        "# ============================================================\n",
        "\n",
        "class MultiTaskDeberta(nn.Module):\n",
        "    def __init__(self, base_model, num_clarity, num_evasion):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = AutoModel.from_pretrained(base_model)\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "\n",
        "        self.clarity_head = nn.Linear(hidden, num_clarity)\n",
        "        self.evasion_head = nn.Linear(hidden, num_evasion)\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "        enc_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls = enc_out.last_hidden_state[:, 0, :]\n",
        "\n",
        "        logits_clarity = self.clarity_head(cls)\n",
        "        logits_evasion = self.evasion_head(cls)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels_clarity = labels[:, 0]\n",
        "            labels_evasion = labels[:, 1]\n",
        "\n",
        "            loss = (self.loss_fn(logits_clarity, labels_clarity) +\n",
        "                    self.loss_fn(logits_evasion, labels_evasion)) / 2.0\n",
        "\n",
        "        # Return a tuple (loss, logits_clarity, logits_evasion) if labels exist\n",
        "        return (loss, logits_clarity, logits_evasion) if loss is not None else (logits_clarity, logits_evasion)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Initializing model...\")\n",
        "model = MultiTaskDeberta(\n",
        "    base_model=model_name,\n",
        "    num_clarity=len(clarity2id),\n",
        "    num_evasion=len(evasion2id)\n",
        ")"
      ],
      "metadata": {
        "id": "Md5_iG7Vj40k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2684efd2-8d43-4d94-885d-30ccee697e6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. Metrics\n",
        "# ============================================================\n",
        "\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    # Unpack the tuple returned by the model\n",
        "    # pred.predictions: (logits_clarity, logits_evasion)\n",
        "    logits_clarity, logits_evasion = pred.predictions\n",
        "\n",
        "    # Convert logits to predicted labels\n",
        "    preds_clarity = np.argmax(logits_clarity, axis=-1)\n",
        "    preds_evasion = np.argmax(logits_evasion, axis=-1)\n",
        "\n",
        "    # True labels\n",
        "    labels_clarity = pred.label_ids[:, 0]\n",
        "    labels_evasion = pred.label_ids[:, 1]\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {\n",
        "        \"clarity_accuracy\": acc.compute(predictions=preds_clarity, references=labels_clarity)[\"accuracy\"],\n",
        "        \"clarity_precision\": precision_score(labels_clarity, preds_clarity, average=\"weighted\", zero_division=0),\n",
        "        \"clarity_recall\": recall_score(labels_clarity, preds_clarity, average=\"weighted\", zero_division=0),\n",
        "        \"clarity_f1\": f1_score(labels_clarity, preds_clarity, average=\"weighted\", zero_division=0),\n",
        "\n",
        "        \"evasion_accuracy\": acc.compute(predictions=preds_evasion, references=labels_evasion)[\"accuracy\"],\n",
        "        \"evasion_precision\": precision_score(labels_evasion, preds_evasion, average=\"weighted\", zero_division=0),\n",
        "        \"evasion_recall\": recall_score(labels_evasion, preds_evasion, average=\"weighted\", zero_division=0),\n",
        "        \"evasion_f1\": f1_score(labels_evasion, preds_evasion, average=\"weighted\", zero_division=0),\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "i5Hd0ZwKj7P9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. Trainer setup\n",
        "# ============================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./clarity_model\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    do_eval = True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded[\"train\"],\n",
        "    eval_dataset=encoded.get(\"validation\", encoded[\"test\"]),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Saving model and trainer state...\")\n",
        "trainer.save_model(\"./clarity_model\")\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "oIqanN4Bj9p8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e5e213fc-a06b-40e4-dd51-f58492b77013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2587' max='2586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2586/2586 29:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Clarity Accuracy</th>\n",
              "      <th>Clarity Precision</th>\n",
              "      <th>Clarity Recall</th>\n",
              "      <th>Clarity F1</th>\n",
              "      <th>Evasion Accuracy</th>\n",
              "      <th>Evasion Precision</th>\n",
              "      <th>Evasion Recall</th>\n",
              "      <th>Evasion F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.341800</td>\n",
              "      <td>2.216249</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.648376</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.611413</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>0.012903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.117500</td>\n",
              "      <td>2.296629</td>\n",
              "      <td>0.685065</td>\n",
              "      <td>0.698186</td>\n",
              "      <td>0.685065</td>\n",
              "      <td>0.675198</td>\n",
              "      <td>0.097403</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.097403</td>\n",
              "      <td>0.177515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.149500</td>\n",
              "      <td>2.625414</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.677487</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.670791</td>\n",
              "      <td>0.058442</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.058442</td>\n",
              "      <td>0.110429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force save trainer state (for plotting)\n",
        "os.makedirs(\"./clarity_model\", exist_ok=True)\n",
        "with open(\"./clarity_model/trainer_state.json\", \"w\") as f:\n",
        "    json.dump(trainer.state.log_history, f)"
      ],
      "metadata": {
        "id": "zBrcWMuo5Zbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. Plot Training Loss and Evaluation Metrics\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nPlotting training curves...\\n\")\n",
        "\n",
        "state_file = \"./clarity_model/trainer_state.json\"\n",
        "\n",
        "if os.path.exists(state_file):\n",
        "    with open(state_file, \"r\") as f:\n",
        "        state = json.load(f)\n",
        "\n",
        "    if isinstance(state, list):\n",
        "        logs = state\n",
        "    else:\n",
        "        logs = state.get(\"log_history\", [])\n",
        "\n",
        "    steps, train_loss = [], []\n",
        "\n",
        "    # Metric lists for clarity\n",
        "    clarity_acc_vals, clarity_prec_vals, clarity_recall_vals, clarity_f1_vals = [], [], [], []\n",
        "\n",
        "    # Metric lists for evasion\n",
        "    evasion_acc_vals, evasion_prec_vals, evasion_recall_vals, evasion_f1_vals = [], [], [], []\n",
        "\n",
        "    for entry in logs:\n",
        "        if \"loss\" in entry and \"step\" in entry:\n",
        "            steps.append(entry[\"step\"])\n",
        "            train_loss.append(entry[\"loss\"])\n",
        "\n",
        "        # Clarity metrics\n",
        "        if \"eval_clarity_accuracy\" in entry:\n",
        "            clarity_acc_vals.append(entry[\"eval_clarity_accuracy\"])\n",
        "        if \"eval_clarity_precision\" in entry:\n",
        "            clarity_prec_vals.append(entry[\"eval_clarity_precision\"])\n",
        "        if \"eval_clarity_recall\" in entry:\n",
        "            clarity_recall_vals.append(entry[\"eval_clarity_recall\"])\n",
        "        if \"eval_clarity_f1\" in entry:\n",
        "            clarity_f1_vals.append(entry[\"eval_clarity_f1\"])\n",
        "\n",
        "        # Evasion metrics\n",
        "        if \"eval_evasion_accuracy\" in entry:\n",
        "            evasion_acc_vals.append(entry[\"eval_evasion_accuracy\"])\n",
        "        if \"eval_evasion_precision\" in entry:\n",
        "            evasion_prec_vals.append(entry[\"eval_evasion_precision\"])\n",
        "        if \"eval_evasion_recall\" in entry:\n",
        "            evasion_recall_vals.append(entry[\"eval_evasion_recall\"])\n",
        "        if \"eval_evasion_f1\" in entry:\n",
        "            evasion_f1_vals.append(entry[\"eval_evasion_f1\"])\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(steps, train_loss, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Over Time\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Clarity metrics\n",
        "    plt.figure(figsize=(10,5))\n",
        "    if clarity_acc_vals: plt.plot(range(1, len(clarity_acc_vals)+1), clarity_acc_vals, label=\"Accuracy\")\n",
        "    if clarity_prec_vals: plt.plot(range(1, len(clarity_prec_vals)+1), clarity_prec_vals, label=\"Precision\")\n",
        "    if clarity_recall_vals: plt.plot(range(1, len(clarity_recall_vals)+1), clarity_recall_vals, label=\"Recall\")\n",
        "    if clarity_f1_vals: plt.plot(range(1, len(clarity_f1_vals)+1), clarity_f1_vals, label=\"F1 Score\")\n",
        "    plt.xlabel(\"Evaluation Epoch\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"Clarity Head Metrics Over Epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Evasion metrics\n",
        "    plt.figure(figsize=(10,5))\n",
        "    if evasion_acc_vals: plt.plot(range(1, len(evasion_acc_vals)+1), evasion_acc_vals, label=\"Accuracy\")\n",
        "    if evasion_prec_vals: plt.plot(range(1, len(evasion_prec_vals)+1), evasion_prec_vals, label=\"Precision\")\n",
        "    if evasion_recall_vals: plt.plot(range(1, len(evasion_recall_vals)+1), evasion_recall_vals, label=\"Recall\")\n",
        "    if evasion_f1_vals: plt.plot(range(1, len(evasion_f1_vals)+1), evasion_f1_vals, label=\"F1 Score\")\n",
        "    plt.xlabel(\"Evaluation Epoch\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"Evasion Head Metrics Over Epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"trainer_state.json not found. Run training first.\")\n"
      ],
      "metadata": {
        "id": "1Bu2sCzYj_qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Inference wrapper\n",
        "# ============================================================\n",
        "\n",
        "def predict(question, answer):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    encoded = tokenizer(\n",
        "        question,\n",
        "        answer,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs)\n",
        "\n",
        "    clarity_id = out[\"logits_clarity\"].argmax(dim=-1).item()\n",
        "    evasion_id = out[\"logits_evasion\"].argmax(dim=-1).item()\n",
        "\n",
        "    return id2clarity[clarity_id], id2evasion[evasion_id]"
      ],
      "metadata": {
        "id": "VHG_6kMNkCYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Examples\n",
        "# ============================================================\n",
        "\n",
        "examples = [\n",
        "    (\"Why did you veto the bill?\",\n",
        "     \"Because the timing was not appropriate.\"),\n",
        "\n",
        "    (\"Can you explain why the health budget was reduced?\",\n",
        "     \"We are looking into multiple ways of improving efficiency.\"),\n",
        "\n",
        "    (\"Do you take responsibility for the outcome?\",\n",
        "     \"Our team has been working very hard and we are committed to improvement.\"),\n",
        "\n",
        "    (\"Why was the water infrastructure project delayed?\",\n",
        "     \"We are currently reviewing contractor proposals to ensure compliance.\"),\n",
        "\n",
        "    (\"Can you explain why the education grant was cut this quarter?\",\n",
        "     \"Several departments are undergoing restructuring to optimize future funding.\"),\n",
        "\n",
        "    (\"Did your office approve the new zoning changes?\",\n",
        "     \"Urban development is a collaborative process, and many stakeholders are involved.\"),\n",
        "\n",
        "    (\"Why haven't the environmental reports been released yet?\",\n",
        "     \"We want to ensure that all findings are thoroughly verified before publication.\"),\n",
        "\n",
        "    (\"Is the transportation department responsible for the recent safety lapses?\",\n",
        "     \"Safety is a shared responsibility and we're evaluating all contributing factors.\"),\n",
        "\n",
        "    (\"Why did you choose not to attend the committee hearing?\",\n",
        "     \"Scheduling conflicts required my attention elsewhere, but my team was present.\"),\n",
        "\n",
        "    (\"Are you planning to revise the energy policy this year?\",\n",
        "     \"We are exploring many promising avenues and will have more to share soon.\"),\n",
        "\n",
        "    (\"Why were the housing subsidies paused?\",\n",
        "     \"We're analyzing long-term impacts to ensure sustainability.\"),\n",
        "\n",
        "    (\"Did the agency conduct the internal audit as required?\",\n",
        "     \"Audits are ongoing, and we'll release findings once the review is complete.\"),\n",
        "\n",
        "    (\"What led to the recent increase in administrative fees?\",\n",
        "     \"The agency is evaluating cost structures to improve service delivery.\")\n",
        "]\n",
        "\n",
        "print(\"\\nExample Predictions:\\n\")\n",
        "for q, a in examples:\n",
        "    c, e = predict(q, a)\n",
        "    print(f\"Q: {q}\\nA: {a}\\nâ†’ Clarity: {c} | Evasion: {e}\\n\")"
      ],
      "metadata": {
        "id": "W_ZcIV0ZkEeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}